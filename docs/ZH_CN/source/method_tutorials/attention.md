# ğŸ¯ DiT æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›ç±»å‹é…ç½®è¯´æ˜

å½“å‰ DiT æ¨¡å‹åœ¨ `LightX2V` ä¸­ä¸‰ä¸ªåœ°æ–¹ä½¿ç”¨åˆ°äº†æ³¨æ„åŠ›ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¯ä»¥åˆ†åˆ«é…ç½®åº•å±‚æ³¨æ„åŠ›åº“ç±»å‹ã€‚

---

## ä½¿ç”¨æ³¨æ„åŠ›çš„ä½ç½®

1. **å›¾åƒçš„è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰**
   - é…ç½®å‚æ•°ï¼š`self_attn_1_type`

2. **å›¾åƒä¸æç¤ºè¯ï¼ˆTextï¼‰ä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰**
   - é…ç½®å‚æ•°ï¼š`cross_attn_1_type`

3. **I2V æ¨¡å¼ä¸‹å›¾åƒä¸å‚è€ƒå›¾ï¼ˆReferenceï¼‰ä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›**
   - é…ç½®å‚æ•°ï¼š`cross_attn_2_type`

---

## ğŸš€ æ”¯æŒçš„æ³¨æ„åŠ›åº“ï¼ˆBackendï¼‰

| åç§°               | ç±»å‹åç§°         | GitHub é“¾æ¥ |
|--------------------|------------------|-------------|
| Flash Attention 2  | `flash_attn2`    | [flash-attention v2](https://github.com/Dao-AILab/flash-attention) |
| Flash Attention 3  | `flash_attn3`    | [flash-attention v3](https://github.com/Dao-AILab/flash-attention) |
| Sage Attention 2   | `sage_attn2`     | [SageAttention](https://github.com/thu-ml/SageAttention) |
| Radial Attention   | `radial_attn`    | [Radial Attention](https://github.com/mit-han-lab/radial-attention) |
| Sparge Attention   | `sparge_ckpt`     | [Sparge Attention](https://github.com/thu-ml/SpargeAttn) |

---

## ğŸ› ï¸ é…ç½®ç¤ºä¾‹

åœ¨ `wan_i2v.json` é…ç½®æ–‡ä»¶ä¸­ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æŒ‡å®šä½¿ç”¨çš„æ³¨æ„åŠ›ç±»å‹ï¼š

```json
{
  "self_attn_1_type": "radial_attn",
  "cross_attn_1_type": "flash_attn3",
  "cross_attn_2_type": "flash_attn3"
}
```

å¦‚éœ€æ›´æ¢ä¸ºå…¶ä»–ç±»å‹ï¼Œåªéœ€å°†å¯¹åº”å€¼æ›¿æ¢ä¸ºä¸Šè¿°è¡¨æ ¼ä¸­çš„ç±»å‹åç§°å³å¯ã€‚

tips: radial_attnå› ä¸ºç¨€ç–ç®—æ³•åŸç†çš„é™åˆ¶åªèƒ½ç”¨åœ¨self attention

---

å¯¹äº Sparge Attention é…ç½®å‚è€ƒ `wan_t2v_sparge.json` æ–‡ä»¶:

    Sparge Attentionæ˜¯éœ€è¦åä¸€ä¸ªè®­ç»ƒçš„æƒé‡

```json
{
  "self_attn_1_type": "flash_attn3",
  "cross_attn_1_type": "flash_attn3",
  "cross_attn_2_type": "flash_attn3"
  "sparge": true,
  "sparge_ckpt": "/path/to/sparge_wan2.1_t2v_1.3B.pt"
}
```

---

å¦‚éœ€è¿›ä¸€æ­¥å®šåˆ¶æ³¨æ„åŠ›æœºåˆ¶çš„è¡Œä¸ºï¼Œè¯·å‚è€ƒå„æ³¨æ„åŠ›åº“çš„å®˜æ–¹æ–‡æ¡£æˆ–å®ç°ä»£ç ã€‚
