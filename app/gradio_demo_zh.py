import os
import gradio as gr
import asyncio
import argparse
import json
import torch
import gc
from easydict import EasyDict
from datetime import datetime
from loguru import logger
import sys
from pathlib import Path

module_path = str(Path(__file__).resolve().parent.parent)
sys.path.append(module_path)

from lightx2v.infer import init_runner
from lightx2v.utils.envs import *

# advance_ptq
logger.add(
    "inference_logs.log",
    rotation="100 MB",
    encoding="utf-8",
    enqueue=True,
    backtrace=True,
    diagnose=True,
)


global_runner = None
current_config = None


def generate_unique_filename(base_dir="./saved_videos"):
    os.makedirs(base_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return os.path.join(base_dir, f"{model_cls}_{timestamp}.mp4")


def run_inference(
    model_type,
    task,
    prompt,
    negative_prompt,
    image_path,
    save_video_path,
    torch_compile,
    infer_steps,
    num_frames,
    resolution,
    seed,
    sample_shift,
    enable_teacache,
    teacache_thresh,
    enable_cfg,
    cfg_scale,
    dit_quant_scheme,
    t5_quant_scheme,
    clip_quant_scheme,
    fps,
    use_tiny_vae,
    use_tiling_vae,
    lazy_load,
    precision_mode,
    use_expandable_alloc,
    cpu_offload,
    offload_granularity,
    t5_offload_granularity,
    attention_type,
    quant_op,
    rotary_chunk,
    clean_cuda_cache,
):
    global global_runner, current_config, model_path

    if os.path.exists(os.path.join(model_path, "config.json")):
        with open(os.path.join(model_path, "config.json"), "r") as f:
            model_config = json.load(f)

    if task == "æ–‡ç”Ÿè§†é¢‘":
        task = "t2v"
    elif task == "å›¾ç”Ÿè§†é¢‘":
        task = "i2v"

    if task == "t2v":
        if model_type == "Wan2.1 1.3B":
            # 1.3B
            coefficient = [
                [
                    -5.21862437e04,
                    9.23041404e03,
                    -5.28275948e02,
                    1.36987616e01,
                    -4.99875664e-02,
                ],
                [
                    2.39676752e03,
                    -1.31110545e03,
                    2.01331979e02,
                    -8.29855975e00,
                    1.37887774e-01,
                ],
            ]
        else:
            # 14B
            coefficient = [
                [
                    -3.03318725e05,
                    4.90537029e04,
                    -2.65530556e03,
                    5.87365115e01,
                    -3.15583525e-01,
                ],
                [
                    -5784.54975374,
                    5449.50911966,
                    -1811.16591783,
                    256.27178429,
                    -13.02252404,
                ],
            ]
    elif task == "i2v":
        if resolution in [
            "1280x720",
            "720x1280",
            "1024x1024",
            "1280x544",
            "544x1280",
            "1104x832",
            "832x1104",
            "960x960",
        ]:
            # 720p
            coefficient = [
                [
                    8.10705460e03,
                    2.13393892e03,
                    -3.72934672e02,
                    1.66203073e01,
                    -4.17769401e-02,
                ],
                [-114.36346466, 65.26524496, -18.82220707, 4.91518089, -0.23412683],
            ]
        else:
            # 480p
            coefficient = [
                [
                    2.57151496e05,
                    -3.54229917e04,
                    1.40286849e03,
                    -1.35890334e01,
                    1.32517977e-01,
                ],
                [
                    -3.02331670e02,
                    2.23948934e02,
                    -5.25463970e01,
                    5.87348440e00,
                    -2.01973289e-01,
                ],
            ]

    save_video_path = generate_unique_filename()

    is_dit_quant = dit_quant_scheme != "bf16"
    is_t5_quant = t5_quant_scheme != "bf16"
    if is_t5_quant:
        if t5_quant_scheme == "int8":
            t5_quant_ckpt = os.path.join(model_path, "models_t5_umt5-xxl-enc-int8.pth")
        else:
            t5_quant_ckpt = os.path.join(model_path, "models_t5_umt5-xxl-enc-fp8.pth")
    else:
        t5_quant_ckpt = None

    is_clip_quant = clip_quant_scheme != "bf16"
    if is_clip_quant:
        if clip_quant_scheme == "int8":
            clip_quant_ckpt = os.path.join(model_path, "clip-int8.pth")
        else:
            clip_quant_ckpt = os.path.join(model_path, "clip-fp8.pth")
    else:
        clip_quant_ckpt = None

    needs_reinit = lazy_load or global_runner is None or current_config is None or current_config.get("model_path") != model_path

    if torch_compile:
        os.environ["ENABLE_GRAPH_MODE"] = "true"
    else:
        os.environ["ENABLE_GRAPH_MODE"] = "false"
    if precision_mode == "bf16":
        os.environ["DTYPE"] = "BF16"
    else:
        os.environ.pop("DTYPE", None)
    if use_expandable_alloc:
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:true"
    else:
        os.environ.pop("PYTORCH_CUDA_ALLOC_CONF", None)

    if is_dit_quant:
        if quant_op == "vllm":
            mm_type = f"W-{dit_quant_scheme}-channel-sym-A-{dit_quant_scheme}-channel-sym-dynamic-Vllm"
        elif quant_op == "sgl":
            mm_type = f"W-{dit_quant_scheme}-channel-sym-A-{dit_quant_scheme}-channel-sym-dynamic-Sgl"
        elif quant_op == "q8f":
            mm_type = f"W-{dit_quant_scheme}-channel-sym-A-{dit_quant_scheme}-channel-sym-dynamic-Q8F"
    else:
        mm_type = "Default"

    config = {
        "infer_steps": infer_steps,
        "target_video_length": num_frames,
        "target_width": int(resolution.split("x")[0]),
        "target_height": int(resolution.split("x")[1]),
        "attention_type": attention_type,
        "seed": seed,
        "enable_cfg": enable_cfg,
        "sample_guide_scale": cfg_scale,
        "sample_shift": sample_shift,
        "cpu_offload": cpu_offload,
        "offload_granularity": offload_granularity,
        "t5_offload_granularity": t5_offload_granularity,
        "dit_quantized_ckpt": model_path if is_dit_quant else None,
        "mm_config": {
            "mm_type": mm_type,
        },
        "fps": fps,
        "feature_caching": "Tea" if enable_teacache else "NoCaching",
        "coefficients": coefficient,
        "use_ret_steps": True,
        "teacache_thresh": teacache_thresh,
        "t5_quantized": is_t5_quant,
        "t5_quantized_ckpt": t5_quant_ckpt,
        "t5_quant_scheme": t5_quant_scheme,
        "clip_quantized": is_clip_quant,
        "clip_quantized_ckpt": clip_quant_ckpt,
        "clip_quant_scheme": clip_quant_scheme,
        "use_tiling_vae": use_tiling_vae,
        "tiny_vae": use_tiny_vae,
        "tiny_vae_path": (os.path.join(model_path, "taew2_1.pth") if use_tiny_vae else None),
        "lazy_load": lazy_load,
        "do_mm_calib": False,
        "parallel_attn_type": None,
        "parallel_vae": False,
        "max_area": False,
        "vae_stride": (4, 8, 8),
        "patch_size": (1, 2, 2),
        "lora_path": None,
        "strength_model": 1.0,
        "use_prompt_enhancer": False,
        "text_len": 512,
        "rotary_chunk": rotary_chunk,
        "clean_cuda_cache": clean_cuda_cache,
    }

    args = argparse.Namespace(
        model_cls=model_cls,
        task=task,
        model_path=model_path,
        prompt_enhancer=None,
        prompt=prompt,
        negative_prompt=negative_prompt,
        image_path=image_path,
        save_video_path=save_video_path,
    )

    config.update({k: v for k, v in vars(args).items()})
    config = EasyDict(config)
    config["mode"] = "infer"
    config.update(model_config)

    print(config)
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
    logger.info(f"æ¨ç†é…ç½®:\n{json.dumps(config, indent=4, ensure_ascii=False)}")

    # åˆå§‹åŒ–æˆ–å¤ç”¨runner
    runner = global_runner
    if needs_reinit:
        if runner is not None:
            del runner
            torch.cuda.empty_cache()
            gc.collect()

        runner = init_runner(config)
        current_config = config

        if not lazy_load:
            global_runner = runner

    asyncio.run(runner.run_pipeline())

    if lazy_load:
        del runner
        torch.cuda.empty_cache()
        gc.collect()

    return save_video_path


def main():
    parser = argparse.ArgumentParser(description="Light Video Generation")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument(
        "--model_cls",
        type=str,
        choices=["wan2.1"],
        default="wan2.1",
        help="ä½¿ç”¨çš„æ¨¡å‹ç±»åˆ«",
    )
    parser.add_argument("--server_port", type=int, default=7862, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--server_name", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åç§°")
    args = parser.parse_args()

    global model_path, model_cls
    model_path = args.model_path
    model_cls = args.model_cls

    def update_model_type(task_type):
        if task_type == "å›¾ç”Ÿè§†é¢‘":
            return gr.update(choices=["Wan2.1 14B"], value="Wan2.1 14B")
        elif task_type == "æ–‡ç”Ÿè§†é¢‘":
            return gr.update(choices=["Wan2.1 14B", "Wan2.1 1.3B"], value="Wan2.1 14B")

    def toggle_image_input(task):
        return gr.update(visible=(task == "å›¾ç”Ÿè§†é¢‘"))

    with gr.Blocks(
        title="Lightx2v(è½»é‡çº§è§†é¢‘æ¨ç†ç”Ÿæˆå¼•æ“)",
        css="""
        .main-content { max-width: 1400px; margin: auto; }
        .output-video { max-height: 650px; }
        .warning { color: #ff6b6b; font-weight: bold; }
        .advanced-options { background: #f9f9ff; border-radius: 10px; padding: 15px; }
        .tab-button { font-size: 16px; padding: 10px 20px; }
    """,
    ) as demo:
        gr.Markdown(f"# ğŸ¬ {model_cls} è§†é¢‘ç”Ÿæˆå™¨")
        gr.Markdown(f"### ä½¿ç”¨æ¨¡å‹: {model_path}")

        with gr.Tabs() as tabs:
            with gr.Tab("åŸºç¡€è®¾ç½®", id=1):
                with gr.Row():
                    with gr.Column(scale=4):
                        with gr.Group():
                            gr.Markdown("## ğŸ“¥ è¾“å…¥å‚æ•°")

                            with gr.Row():
                                task = gr.Dropdown(
                                    choices=["å›¾ç”Ÿè§†é¢‘", "æ–‡ç”Ÿè§†é¢‘"],
                                    value="å›¾ç”Ÿè§†é¢‘",
                                    label="ä»»åŠ¡ç±»å‹",
                                )
                                model_type = gr.Dropdown(
                                    choices=["Wan2.1 14B"],
                                    value="Wan2.1 14B",
                                    label="æ¨¡å‹ç±»å‹",
                                )
                                task.change(
                                    fn=update_model_type,
                                    inputs=task,
                                    outputs=model_type,
                                )

                            with gr.Row():
                                image_path = gr.Image(
                                    label="è¾“å…¥å›¾ç‰‡",
                                    type="filepath",
                                    height=300,
                                    interactive=True,
                                    visible=True,  # Initially visible
                                )

                                task.change(
                                    fn=toggle_image_input,
                                    inputs=task,
                                    outputs=image_path,
                                )

                            with gr.Row():
                                with gr.Column():
                                    prompt = gr.Textbox(
                                        label="æç¤ºè¯",
                                        lines=3,
                                        placeholder="æè¿°è§†é¢‘å†…å®¹...",
                                        max_lines=5,
                                    )
                                with gr.Column():
                                    negative_prompt = gr.Textbox(
                                        label="è´Ÿå‘æç¤ºè¯",
                                        lines=3,
                                        placeholder="ä¸å¸Œæœ›è§†é¢‘å‡ºç°çš„å†…å®¹...",
                                        max_lines=5,
                                        value="é•œå¤´æ™ƒåŠ¨ï¼Œè‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
                                    )
                                with gr.Column():
                                    resolution = gr.Dropdown(
                                        choices=[
                                            # 720p
                                            ("1280x720 (16:9, 720p)", "1280x720"),
                                            ("720x1280 (9:16, 720p)", "720x1280"),
                                            ("1024x1024 (1:1, 720p)", "1024x1024"),
                                            ("1280x544 (21:9, 720p)", "1280x544"),
                                            ("544x1280 (9:21, 720p)", "544x1280"),
                                            ("1104x832 (4:3, 720p)", "1104x832"),
                                            ("832x1104 (3:4, 720p)", "832x1104"),
                                            ("960x960 (1:1, 720p)", "960x960"),
                                            # 480p
                                            ("960x544 (16:9, 540p)", "960x544"),
                                            ("544x960 (9:16, 540p)", "544x960"),
                                            ("832x480 (16:9, 480p)", "832x480"),
                                            ("480x832 (9:16, 480p)", "480x832"),
                                            ("832x624 (4:3, 480p)", "832x624"),
                                            ("624x832 (3:4, 480p)", "624x832"),
                                            ("720x720 (1:1, 480p)", "720x720"),
                                            ("512x512 (1:1, 480p)", "512x512"),
                                        ],
                                        value="832x480",
                                        label="æœ€å¤§åˆ†è¾¨ç‡",
                                    )
                                with gr.Column():
                                    seed = gr.Slider(
                                        label="éšæœºç§å­",
                                        minimum=-10000000,
                                        maximum=10000000,
                                        step=1,
                                        value=42,
                                        info="å›ºå®šéšæœºç§å­ä»¥è·å¾—å¯å¤ç°çš„ç»“æœ",
                                    )
                                    infer_steps = gr.Slider(
                                        label="æ¨ç†æ­¥æ•°",
                                        minimum=1,
                                        maximum=100,
                                        step=1,
                                        value=20,
                                        info="è§†é¢‘ç”Ÿæˆçš„æ¨ç†æ­¥æ•°ï¼Œå¢åŠ æ­¥æ•°å¯èƒ½æé«˜è´¨é‡ä½†ä¼šé™ä½é€Ÿåº¦",
                                    )
                                    sample_shift = gr.Slider(
                                        label="åˆ†å¸ƒåç§»ç¨‹åº¦",
                                        value=5,
                                        minimum=0,
                                        maximum=10,
                                        step=1,
                                        info="ç”¨äºæ§åˆ¶æ ·æœ¬çš„åˆ†å¸ƒåç§»ç¨‹åº¦ï¼Œæ•°å€¼è¶Šå¤§è¡¨ç¤ºåç§»è¶Šæ˜æ˜¾",
                                    )

                                fps = gr.Slider(
                                    label="å¸§ç‡(FPS)",
                                    minimum=8,
                                    maximum=30,
                                    step=1,
                                    value=16,
                                    info="è§†é¢‘æ¯ç§’å¸§æ•°ï¼Œæ›´é«˜çš„FPSç”Ÿæˆæ›´æµç•…çš„è§†é¢‘",
                                )
                                num_frames = gr.Slider(
                                    label="æ€»å¸§æ•°",
                                    minimum=16,
                                    maximum=120,
                                    step=1,
                                    value=81,
                                    info="è§†é¢‘æ€»å¸§æ•°ï¼Œæ›´å¤šçš„å¸§æ•°ç”Ÿæˆæ›´é•¿çš„è§†é¢‘",
                                )

                            save_video_path = gr.Textbox(
                                label="è¾“å‡ºè§†é¢‘è·¯å¾„",
                                value=generate_unique_filename(),
                                info="å¿…é¡»åŒ…å«.mp4åç¼€ï¼Œå¦‚æœç•™ç©ºæˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å",
                            )

                            infer_btn = gr.Button("ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")

                    with gr.Column(scale=6):
                        gr.Markdown("## ğŸ“¤ ç”Ÿæˆçš„è§†é¢‘")
                        output_video = gr.Video(
                            label="ç»“æœ",
                            height=624,
                            width=360,
                            autoplay=True,
                            elem_classes=["output-video"],
                        )

            with gr.Tab("âš™ï¸ é«˜çº§é€‰é¡¹", id=2):
                with gr.Group(elem_classes="advanced-options"):
                    gr.Markdown("### æ— åˆ†ç±»å™¨å¼•å¯¼(CFG)")
                    with gr.Row():
                        enable_cfg = gr.Checkbox(
                            label="å¯ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼",
                            value=False,
                            info="å¯ç”¨åˆ†ç±»å™¨å¼•å¯¼ï¼Œç”¨äºæ§åˆ¶æç¤ºè¯å¼ºåº¦",
                        )
                        cfg_scale = gr.Slider(
                            label="CFGç¼©æ”¾ç³»æ•°",
                            minimum=1,
                            maximum=100,
                            step=1,
                            value=5,
                            info="æ§åˆ¶æç¤ºè¯çš„å½±å“å¼ºåº¦ï¼Œå€¼è¶Šé«˜æç¤ºè¯å½±å“è¶Šå¤§",
                        )

                    gr.Markdown("### æ˜¾å­˜/å†…å­˜ä¼˜åŒ–")
                    with gr.Row():
                        lazy_load = gr.Checkbox(
                            label="å¯ç”¨å»¶è¿ŸåŠ è½½",
                            value=False,
                            info="æ¨ç†æ—¶å»¶è¿ŸåŠ è½½æ¨¡å‹ç»„ä»¶ï¼Œé€‚ç”¨å†…å­˜å—é™ç¯å¢ƒ",
                        )

                        torch_compile = gr.Checkbox(
                            label="å¯ç”¨Torchç¼–è¯‘",
                            value=False,
                            info="ä½¿ç”¨torch.compileåŠ é€Ÿæ¨ç†è¿‡ç¨‹",
                        )

                        use_expandable_alloc = gr.Checkbox(
                            label="å¯ç”¨å¯æ‰©å±•æ˜¾å­˜åˆ†é…",
                            value=False,
                            info="æœ‰åŠ©äºå‡å°‘æ˜¾å­˜ç¢ç‰‡",
                        )

                        rotary_chunk = gr.Checkbox(
                            label="åˆ†å—å¤„ç†æ—‹è½¬ä½ç½®ç¼–ç ",
                            value=False,
                            info="å¯ç”¨åï¼Œä½¿ç”¨åˆ†å—å¤„ç†æ—‹è½¬ä½ç½®ç¼–ç èŠ‚çœæ˜¾å­˜ã€‚",
                        )

                        clean_cuda_cache = gr.Checkbox(
                            label="æ¸…ç† CUDA æ˜¾å­˜ç¼“å­˜",
                            value=False,
                            info="å¯ç”¨åï¼ŒåŠæ—¶é‡Šæ”¾æ˜¾å­˜ä½†æ¨ç†é€Ÿåº¦å˜æ…¢ã€‚",
                        )

                    with gr.Row():
                        cpu_offload = gr.Checkbox(
                            label="CPUå¸è½½",
                            value=False,
                            info="å°†æ¨¡å‹çš„éƒ¨åˆ†è®¡ç®—ä» GPU å¸è½½åˆ° CPUï¼Œä»¥é™ä½æ˜¾å­˜å ç”¨",
                        )
                        offload_granularity = gr.Dropdown(
                            label="Dit å¸è½½ç²’åº¦",
                            choices=["block", "phase"],
                            value="block",
                            info="æ§åˆ¶ Dit æ¨¡å‹å¸è½½åˆ° CPU æ—¶çš„ç²’åº¦",
                        )
                        t5_offload_granularity = gr.Dropdown(
                            label="T5 Encoder å¸è½½ç²’åº¦",
                            choices=["model", "block"],
                            value="block",
                            info="æ§åˆ¶ T5 Encoder æ¨¡å‹å¸è½½åˆ° CPU æ—¶çš„ç²’åº¦",
                        )

                    gr.Markdown("### ä½ç²¾åº¦é‡åŒ–")
                    with gr.Row():
                        attention_type = gr.Dropdown(
                            label="attention ç®—å­",
                            choices=["flash_attn2", "flash_attn3", "sage_attn2"],
                            value="flash_attn2",
                            info="ä½¿ç”¨åˆé€‚çš„ attention ç®—å­å¯åŠ é€Ÿæ¨ç†",
                        )

                        quant_op = gr.Dropdown(
                            label="é‡åŒ–ç®—å­",
                            choices=["vllm", "sgl", "q8f"],
                            value="vllm",
                            info="ä½¿ç”¨åˆé€‚çš„é‡åŒ–ç®—å­å¯åŠ é€Ÿæ¨ç†",
                        )

                        dit_quant_scheme = gr.Dropdown(
                            label="Dit",
                            choices=["fp8", "int8", "bf16"],
                            value="bf16",
                            info="Ditæ¨¡å‹çš„é‡åŒ–ç²¾åº¦",
                        )
                        t5_quant_scheme = gr.Dropdown(
                            label="T5 Encoder",
                            choices=["fp8", "int8", "bf16"],
                            value="bf16",
                            info="T5 Encoderæ¨¡å‹çš„é‡åŒ–ç²¾åº¦",
                        )
                        clip_quant_scheme = gr.Dropdown(
                            label="Clip Encoder",
                            choices=["fp8", "int8", "fp16"],
                            value="fp16",
                            info="Clip Encoderçš„é‡åŒ–ç²¾åº¦",
                        )
                        precision_mode = gr.Dropdown(
                            label="æ•æ„Ÿå±‚ç²¾åº¦",
                            choices=["fp32", "bf16"],
                            value="bf16",
                            info="é€‰æ‹©ç”¨äºæ•æ„Ÿå±‚è®¡ç®—çš„æ•°å€¼ç²¾åº¦ã€‚",
                        )

                    gr.Markdown("### å˜åˆ†è‡ªç¼–ç å™¨(VAE)")
                    with gr.Row():
                        use_tiny_vae = gr.Checkbox(
                            label="ä½¿ç”¨è½»é‡çº§VAE",
                            value=False,
                            info="ä½¿ç”¨è½»é‡çº§VAEæ¨¡å‹åŠ é€Ÿè§£ç è¿‡ç¨‹",
                        )
                        use_tiling_vae = gr.Checkbox(
                            label="å¯ç”¨ VAE å¹³é“ºæ¨ç†",
                            value=False,
                            info="ä½¿ç”¨ VAE å¹³é“ºæ¨ç†ä»¥é™ä½æ˜¾å­˜å ç”¨",
                        )

                    gr.Markdown("### ç‰¹å¾ç¼“å­˜")
                    with gr.Row():
                        enable_teacache = gr.Checkbox(
                            label="å¯ç”¨Tea Cache",
                            value=False,
                            info="åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼“å­˜ç‰¹å¾ä»¥å‡å°‘æ¨ç†æ­¥æ•°",
                        )
                        teacache_thresh = gr.Slider(
                            label="Tea Cacheé˜ˆå€¼",
                            value=0.26,
                            minimum=0,
                            maximum=1,
                            info="åŠ é€Ÿè¶Šé«˜ï¼Œè´¨é‡å¯èƒ½è¶Šå·® â€”â€” è®¾ç½®ä¸º 0.1 å¯è·å¾—çº¦ 2.0 å€åŠ é€Ÿï¼Œè®¾ç½®ä¸º 0.2 å¯è·å¾—çº¦ 3.0 å€åŠ é€Ÿ",
                        )

        infer_btn.click(
            fn=run_inference,
            inputs=[
                model_type,
                task,
                prompt,
                negative_prompt,
                image_path,
                save_video_path,
                torch_compile,
                infer_steps,
                num_frames,
                resolution,
                seed,
                sample_shift,
                enable_teacache,
                teacache_thresh,
                enable_cfg,
                cfg_scale,
                dit_quant_scheme,
                t5_quant_scheme,
                clip_quant_scheme,
                fps,
                use_tiny_vae,
                use_tiling_vae,
                lazy_load,
                precision_mode,
                use_expandable_alloc,
                cpu_offload,
                offload_granularity,
                t5_offload_granularity,
                attention_type,
                quant_op,
                rotary_chunk,
                clean_cuda_cache,
            ],
            outputs=output_video,
        )

    demo.launch(share=True, server_port=args.server_port, server_name=args.server_name)


if __name__ == "__main__":
    main()
